{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "from tqdm import tqdm\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_documents(filename):\n",
    "    with open(filename+'.txt','r',encoding='utf-8') as l:\n",
    "        lemmas = l.read().split('\\n')\n",
    "    l.close()\n",
    "    tokens = tokenizer.tokenize_sents(lemmas)\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)]\n",
    "    return lemmas, tokens, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_tokens, train_documents = acquire_documents('Train_lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(train_documents, vector_size=dim, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Train_only.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87719/87719 [01:35<00:00, 920.37it/s] \n"
     ]
    }
   ],
   "source": [
    "train_vectors = [model.infer_vector(i.words) for i in tqdm(train_documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts, test_tokens, test_documents = acquire_documents('Test_lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:01<00:00, 880.53it/s]\n"
     ]
    }
   ],
   "source": [
    "test_vectors = [model.infer_vector(i.words) for i in tqdm(test_documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts, val_tokens, val_documents = acquire_documents('Val_lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:01<00:00, 877.74it/s]\n"
     ]
    }
   ],
   "source": [
    "val_vectors = [model.infer_vector(i.words) for i in tqdm(val_documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top_threehundred_sorted_cutathundred.pkl', 'rb') as t:\n",
    "    top_word_sets = {k:set(val[:300]) for k, val in (pkl.load(t)).items()}\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'алхимик',\n",
       " 'анимация',\n",
       " 'аниме',\n",
       " 'анимешник',\n",
       " 'анимэ',\n",
       " 'арка',\n",
       " 'берсерка',\n",
       " 'бибоп',\n",
       " 'блич',\n",
       " 'боевой',\n",
       " 'вампир',\n",
       " 'гиас',\n",
       " 'дворецкий',\n",
       " 'девочка',\n",
       " 'длинный',\n",
       " 'досматривать',\n",
       " 'дубляж',\n",
       " 'ева',\n",
       " 'евангелиона',\n",
       " 'замок',\n",
       " 'заполнять',\n",
       " 'затягивать',\n",
       " 'кп',\n",
       " 'лайт',\n",
       " 'манга',\n",
       " 'мангак',\n",
       " 'мех',\n",
       " 'меч',\n",
       " 'миядзаки',\n",
       " 'мозг',\n",
       " 'мувик',\n",
       " 'мульт',\n",
       " 'мультик',\n",
       " 'мультфильм',\n",
       " 'нарут',\n",
       " 'наруто',\n",
       " 'необычно',\n",
       " 'объектив',\n",
       " 'овашка',\n",
       " 'озвучка',\n",
       " 'онгоинг',\n",
       " 'опенинг',\n",
       " 'отсмотреть',\n",
       " 'охотиться',\n",
       " 'палатка',\n",
       " 'пересматривать',\n",
       " 'повседневность',\n",
       " 'подросток',\n",
       " 'подсказывать',\n",
       " 'покемон',\n",
       " 'полнометражка',\n",
       " 'полнометражный',\n",
       " 'призрак',\n",
       " 'продолжение',\n",
       " 'просматривать',\n",
       " 'просмотр',\n",
       " 'развиваться',\n",
       " 'раскрывать',\n",
       " 'рисовка',\n",
       " 'романтика',\n",
       " 'самурай',\n",
       " 'седзе',\n",
       " 'сейя',\n",
       " 'сенэн',\n",
       " 'сериал',\n",
       " 'синкай',\n",
       " 'сиська',\n",
       " 'создатель',\n",
       " 'сосед',\n",
       " 'тайтлы',\n",
       " 'тетрадка',\n",
       " 'тетрадь',\n",
       " 'том',\n",
       " 'ух',\n",
       " 'фансервис',\n",
       " 'филлер',\n",
       " 'хвост',\n",
       " 'хентай',\n",
       " 'ценить',\n",
       " 'штамп',\n",
       " 'штука',\n",
       " 'эльфийский',\n",
       " 'эндинг',\n",
       " 'юмор',\n",
       " 'японец',\n",
       " 'япония',\n",
       " 'японский'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_sets['anime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categs = ['anime', 'art', 'books', 'films', 'food', 'football', 'games', 'music', 'nature', 'travel']\n",
    "# weights = [len(v) / max([len(v) for v in top_sets.values()]) for v in top_sets.values()] # normal weights per quantity\n",
    "weights = [1] * 10 # non-existent weights\n",
    "\n",
    "def words_from_tokens(tokens):\n",
    "    vec = [0] * 10\n",
    "    for i in range(len(categs)):\n",
    "        count = 0\n",
    "        for t in tokens:\n",
    "            if t in top_word_sets[categs[i]]:\n",
    "                count += 1\n",
    "        vec[i] = (count / len(tokens))/weights[i]\n",
    "    vec = np.array(vec, dtype='float32')\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_from_corpus(corpus):\n",
    "    vectors = [words_from_tokens(text) for text in tqdm(corpus)]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87719/87719 [00:07<00:00, 11096.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_word_vectors = vectors_from_corpus(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 10277.01it/s]\n"
     ]
    }
   ],
   "source": [
    "test_word_vectors = vectors_from_corpus(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:00<00:00, 10256.45it/s]\n"
     ]
    }
   ],
   "source": [
    "val_word_vectors = vectors_from_corpus(val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigrams_threehundred_sorted.pkl', 'rb') as t:\n",
    "    top_tri_sets = {k:set(val[:300]) for k, val in (pkl.load(t)).items()}\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "categs = ['anime', 'art', 'books', 'films', 'food', 'football', 'games', 'music', 'nature', 'travel']\n",
    "# weights = [len(v) / max([len(v) for v in top_sets.values()]) for v in top_sets.values()] # normal weights per quantity\n",
    "weights = [1] * 10 # non-existent weights\n",
    "\n",
    "def tris_from_text(text):\n",
    "    vec = [0] * 10\n",
    "    for i in range(len(categs)):\n",
    "        count = 0\n",
    "        for trig in top_tri_sets[categs[i]]:\n",
    "            if trig in text:\n",
    "                count += 1\n",
    "        vec[i] = (count / len(text))/weights[i]\n",
    "    vec = np.array(vec, dtype='float32')\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_from_corpus(corpus):\n",
    "    trigrams = [tris_from_text(text) for text in tqdm(corpus)]\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87719/87719 [01:54<00:00, 766.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tri_vectors = trigrams_from_corpus(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:01<00:00, 711.39it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tri_vectors = trigrams_from_corpus(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:01<00:00, 712.23it/s]\n"
     ]
    }
   ],
   "source": [
    "val_tri_vectors = trigrams_from_corpus(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_vectors(vectors, word_vectors, tri_vectors):\n",
    "    joint = [np.concatenate([vectors[i], word_vectors[i], tri_vectors[i]]) for i in tqdm(range(len(vectors)))]\n",
    "    return np.array(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87719/87719 [00:00<00:00, 186101.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train = joint_vectors(train_vectors, train_word_vectors, train_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 132246.87it/s]\n"
     ]
    }
   ],
   "source": [
    "test = joint_vectors(test_vectors, test_word_vectors, test_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:00<00:00, 131624.28it/s]\n"
     ]
    }
   ],
   "source": [
    "val = joint_vectors(val_vectors, val_word_vectors, val_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Train_'+str(dim+20)+'.pkl', 'wb') as tr:\n",
    "    pkl.dump(train, tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Test_'+str(dim+20)+'.pkl', 'wb') as tst:\n",
    "    pkl.dump(test, tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Val_'+str(dim+20)+'.pkl', 'wb') as vl:\n",
    "    pkl.dump(val, vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_vectors_tris(vectors, tri_vectors):\n",
    "    joint = [np.concatenate([vectors[i], tri_vectors[i]]) for i in tqdm(range(len(vectors)))]\n",
    "    return np.array(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87719/87719 [00:00<00:00, 207506.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tri = joint_vectors_tris(train_vectors, train_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 177051.88it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tri = joint_vectors_tris(test_vectors, test_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:00<00:00, 128932.28it/s]\n"
     ]
    }
   ],
   "source": [
    "val_tri = joint_vectors_tris(val_vectors, val_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Train_'+str(dim+10)+'_tris.pkl', 'wb') as tr1:\n",
    "    pkl.dump(train_tri, tr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Test_'+str(dim+10)+'_tris.pkl', 'wb') as tst1:\n",
    "    pkl.dump(test_tri, tst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Val_'+str(dim+10)+'_tris.pkl', 'wb') as vl1:\n",
    "    pkl.dump(val_tri, vl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_vectors_twenty(word_vectors, tri_vectors):\n",
    "    joint = [np.concatenate([word_vectors[i], tri_vectors[i]]) for i in tqdm(range(len(word_vectors)))]\n",
    "    return np.array(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87719/87719 [00:00<00:00, 312972.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tw = joint_vectors_tris(train_word_vectors, train_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 118838.02it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tw = joint_vectors_tris(test_word_vectors, test_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:00<00:00, 167691.51it/s]\n"
     ]
    }
   ],
   "source": [
    "val_tw = joint_vectors_tris(val_word_vectors, val_tri_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Train_twenty.pkl', 'wb') as tw1:\n",
    "    pkl.dump(train_tw, tw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Test_twenty.pkl', 'wb') as tw2:\n",
    "    pkl.dump(test_tw, tw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Val_twenty.pkl', 'wb') as tw3:\n",
    "    pkl.dump(val_tw, tw3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
